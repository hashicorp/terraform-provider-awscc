
---
page_title: "awscc_sagemaker_pipeline Resource - terraform-provider-awscc"
subcategory: ""
description: |-
  Resource Type definition for AWS::SageMaker::Pipeline
---

# awscc_sagemaker_pipeline (Resource)

Resource Type definition for AWS::SageMaker::Pipeline

## Example Usage

### SageMaker Pipeline with Processing Step

Creates a SageMaker Pipeline with a single processing step using scikit-learn container, complete with IAM role configuration and necessary permissions for pipeline execution.

~> This example is generated by LLM using Amazon Bedrock and validated using terraform validate, apply and destroy. While we strive for accuracy and quality, please note that the information provided may not be entirely error-free or up-to-date. We recommend independently verifying the content.

```terraform
# Get current AWS account ID and region
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}

# IAM role for SageMaker Pipeline
data "aws_iam_policy_document" "assume_role" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["sagemaker.amazonaws.com"]
    }
  }
}

data "aws_iam_policy_document" "pipeline_policy" {
  statement {
    effect = "Allow"
    actions = [
      "sagemaker:CreateTrainingJob",
      "sagemaker:CreateProcessingJob",
      "sagemaker:CreateModelPackage",
      "sagemaker:StopProcessingJob",
      "sagemaker:StopTrainingJob"
    ]
    resources = [
      "arn:aws:sagemaker:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:*"
    ]
  }

  statement {
    effect = "Allow"
    actions = [
      "s3:GetObject",
      "s3:PutObject",
      "s3:DeleteObject",
      "s3:ListBucket"
    ]
    resources = [
      "arn:aws:s3:::sagemaker-${data.aws_region.current.name}-${data.aws_caller_identity.current.account_id}/*",
      "arn:aws:s3:::sagemaker-${data.aws_region.current.name}-${data.aws_caller_identity.current.account_id}"
    ]
  }

  statement {
    effect = "Allow"
    actions = [
      "logs:CreateLogGroup",
      "logs:CreateLogStream",
      "logs:PutLogEvents"
    ]
    resources = ["arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:/aws/sagemaker/*"]
  }
}

resource "awscc_iam_role" "sagemaker_pipeline_role" {
  role_name                   = "sagemaker-pipeline-role"
  assume_role_policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.assume_role.json))
  managed_policy_arns         = ["arn:aws:iam::aws:policy/AmazonSageMakerFullAccess"]
  policies = [{
    policy_name     = "sagemaker-pipeline-policy"
    policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.pipeline_policy.json))
  }]
}

# SageMaker Pipeline
resource "awscc_sagemaker_pipeline" "example" {
  pipeline_name = "example-pipeline"
  role_arn      = awscc_iam_role.sagemaker_pipeline_role.arn

  pipeline_definition = {
    pipeline_definition_body = jsonencode({
      Version = "2020-12-01"
      Parameters = [
        {
          Name = "InputDataUrl"
          Type = "String"
        }
      ]
      Steps = [
        {
          Name = "Processing"
          Type = "Processing"
          Arguments = {
            ProcessingResources = {
              ClusterConfig = {
                InstanceCount  = 1
                InstanceType   = "ml.m5.xlarge"
                VolumeSizeInGB = 30
              }
            }
            AppSpecification = {
              ImageUri = "${data.aws_caller_identity.current.account_id}.dkr.ecr.${data.aws_region.current.name}.amazonaws.com/sagemaker-scikit-learn:0.23-1-cpu-py3"
              ContainerArguments = [
                "--input-data", "InputDataUrl"
              ]
            }
            ProcessingInputs = []
            ProcessingOutputConfig = {
              Outputs = []
            }
            RoleArn = awscc_iam_role.sagemaker_pipeline_role.arn
          }
        }
      ]
    })
  }

  pipeline_description = "Example SageMaker Pipeline using Terraform AWSCC provider"

  parallelism_configuration = {
    max_parallel_execution_steps = 10
  }

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `pipeline_definition` (Attributes) (see [below for nested schema](#nestedatt--pipeline_definition))
- `pipeline_name` (String) The name of the Pipeline.
- `role_arn` (String) Role Arn

### Optional

- `parallelism_configuration` (Attributes) (see [below for nested schema](#nestedatt--parallelism_configuration))
- `pipeline_description` (String) The description of the Pipeline.
- `pipeline_display_name` (String) The display name of the Pipeline.
- `tags` (Attributes List) (see [below for nested schema](#nestedatt--tags))

### Read-Only

- `id` (String) Uniquely identifies the resource.

<a id="nestedatt--pipeline_definition"></a>
### Nested Schema for `pipeline_definition`

Optional:

- `pipeline_definition_body` (String) A specification that defines the pipeline in JSON format.
- `pipeline_definition_s3_location` (Attributes) (see [below for nested schema](#nestedatt--pipeline_definition--pipeline_definition_s3_location))

<a id="nestedatt--pipeline_definition--pipeline_definition_s3_location"></a>
### Nested Schema for `pipeline_definition.pipeline_definition_s3_location`

Optional:

- `bucket` (String) The name of the S3 bucket where the PipelineDefinition file is stored.
- `e_tag` (String) The Amazon S3 ETag (a file checksum) of the PipelineDefinition file. If you don't specify a value, SageMaker skips ETag validation of your PipelineDefinition file.
- `key` (String) The file name of the PipelineDefinition file (Amazon S3 object name).
- `version` (String) For versioning-enabled buckets, a specific version of the PipelineDefinition file.



<a id="nestedatt--parallelism_configuration"></a>
### Nested Schema for `parallelism_configuration`

Optional:

- `max_parallel_execution_steps` (Number) Maximum parallel execution steps


<a id="nestedatt--tags"></a>
### Nested Schema for `tags`

Optional:

- `key` (String)
- `value` (String)

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
$ terraform import awscc_sagemaker_pipeline.example "pipeline_name"
```
