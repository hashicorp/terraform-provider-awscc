---
page_title: "awscc_emr_step Resource - terraform-provider-awscc"
subcategory: ""
description: |-
  Schema for AWS::EMR::Step
---

# awscc_emr_step (Resource)

Schema for AWS::EMR::Step

## Example Usage

### EMR Step with Spark Job
EMR Step configuration with complete EMR cluster setup, VPC infrastructure, IAM roles, and Spark job execution

~> This example is generated by LLM using Amazon Bedrock and validated using terraform validate, apply and destroy. While we strive for accuracy and quality, please note that the information provided may not be entirely error-free or up-to-date. We recommend independently verifying the content.

```terraform
# Create an S3 bucket for EMR logs and scripts
resource "aws_s3_bucket" "emr_bucket" {
  bucket        = "example-emr-bucket"
  force_destroy = true
}

# Create a VPC
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_support   = true
  enable_dns_hostnames = true
  tags = {
    Name = "example-vpc"
  }
}

# Create a security group for EMR
resource "aws_security_group" "emr_sg" {
  name        = "emr-security-group"
  description = "Security group for EMR cluster"
  vpc_id      = aws_vpc.main.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

# Create a subnet in the VPC
resource "aws_subnet" "main" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.1.0/24"
  availability_zone       = "us-west-2a"
  map_public_ip_on_launch = true
  tags = {
    Name = "example-subnet"
  }
}

# Create an internet gateway
resource "aws_internet_gateway" "gw" {
  vpc_id = aws_vpc.main.id
  tags = {
    Name = "example-igw"
  }
}

# Create a route table
resource "aws_route_table" "rt" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.gw.id
  }

  tags = {
    Name = "example-route-table"
  }
}

# Associate the route table with the subnet
resource "aws_route_table_association" "a" {
  subnet_id      = aws_subnet.main.id
  route_table_id = aws_route_table.rt.id
}

# Create an IAM role for EMR service
resource "aws_iam_role" "emr_service_role" {
  name = "EMR_ServiceRole"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "elasticmapreduce.amazonaws.com"
        }
      }
    ]
  })
}

# Attach the EMR service policy to the role
resource "aws_iam_role_policy_attachment" "emr_service_role_policy" {
  role       = aws_iam_role.emr_service_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceRole"
}

# Create an IAM role for EC2 instance profile
resource "aws_iam_role" "emr_ec2_role" {
  name = "EMR_EC2_Role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "ec2.amazonaws.com"
        }
      }
    ]
  })
}

# Attach policies to the EC2 role
resource "aws_iam_role_policy_attachment" "emr_ec2_role_policy" {
  role       = aws_iam_role.emr_ec2_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonElasticMapReduceforEC2Role"
}

# Create an EC2 instance profile
resource "aws_iam_instance_profile" "emr_ec2_instance_profile" {
  name = "EMR_EC2_InstanceProfile"
  role = aws_iam_role.emr_ec2_role.name
}

# Create EMR cluster
resource "aws_emr_cluster" "example_cluster" {
  name          = "example-emr-cluster"
  release_label = "emr-6.6.0"
  applications  = ["Spark", "Hadoop"]

  ec2_attributes {
    subnet_id                         = aws_subnet.main.id
    emr_managed_master_security_group = aws_security_group.emr_sg.id
    emr_managed_slave_security_group  = aws_security_group.emr_sg.id
    instance_profile                  = aws_iam_instance_profile.emr_ec2_instance_profile.arn
  }

  master_instance_group {
    instance_type = "m5.xlarge"
  }

  core_instance_group {
    instance_type  = "m5.xlarge"
    instance_count = 1
  }

  service_role = aws_iam_role.emr_service_role.arn

  log_uri = "s3://${aws_s3_bucket.emr_bucket.bucket}/logs/"

  tags = {
    Name = "example-emr-cluster"
  }

  lifecycle {
    ignore_changes = [step]
  }

  depends_on = [
    aws_subnet.main,
    aws_iam_role_policy_attachment.emr_service_role_policy,
    aws_iam_role_policy_attachment.emr_ec2_role_policy,
    aws_iam_instance_profile.emr_ec2_instance_profile,
    aws_route_table_association.a
  ]
}

# Upload a sample jar to S3
resource "aws_s3_object" "example_jar" {
  bucket      = aws_s3_bucket.emr_bucket.id
  key         = "jobs/example-job.jar"
  source      = "example-job.jar"
  source_hash = filemd5("example-job.jar")
}

# EMR Step resource using AWS CloudControl provider
resource "awscc_emr_step" "example" {
  job_flow_id       = aws_emr_cluster.example_cluster.id
  name              = "example-step"
  action_on_failure = "CONTINUE"

  hadoop_jar_step = {
    jar = "command-runner.jar"
    args = [
      "spark-submit",
      "--deploy-mode", "cluster",
      "--class", "org.example.SparkJob",
      "s3://${aws_s3_bucket.emr_bucket.bucket}/jobs/example-job.jar",
      "arg1", "arg2"
    ]
  }

  depends_on = [aws_s3_object.example_jar]
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `action_on_failure` (String) This specifies what action to take when the cluster step fails. Possible values are CANCEL_AND_WAIT and CONTINUE.
- `hadoop_jar_step` (Attributes) The HadoopJarStepConfig property type specifies a job flow step consisting of a JAR file whose main function will be executed. The main function submits a job for the cluster to execute as a step on the master node, and then waits for the job to finish or fail before executing subsequent steps. (see [below for nested schema](#nestedatt--hadoop_jar_step))
- `job_flow_id` (String) A string that uniquely identifies the cluster (job flow).
- `name` (String) The name of the cluster step.

### Optional

- `encryption_key_arn` (String) The KMS key ARN to encrypt the logs published to the given Amazon S3 destination. When omitted, EMR falls back to cluster-level logging behavior.
- `log_uri` (String) The Amazon S3 destination URI for log publishing. When omitted, EMR falls back to cluster-level logging behavior.

### Read-Only

- `id` (String) Uniquely identifies the resource.
- `step_id` (String) ID generated by service

<a id="nestedatt--hadoop_jar_step"></a>
### Nested Schema for `hadoop_jar_step`

Required:

- `jar` (String)

Optional:

- `args` (List of String)
- `main_class` (String)
- `step_properties` (Attributes List) (see [below for nested schema](#nestedatt--hadoop_jar_step--step_properties))

<a id="nestedatt--hadoop_jar_step--step_properties"></a>
### Nested Schema for `hadoop_jar_step.step_properties`

Optional:

- `key` (String)
- `value` (String)

## Import

Import is supported using the following syntax:

In Terraform v1.12.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `identity` attribute, for example:

```terraform
import {
  to = awscc_emr_step.example
  identity = {
    id = "id"
  }
}
```

<!-- schema generated by tfplugindocs -->
### Identity Schema

#### Required

- `id` (String) ID generated by service

#### Optional

- `account_id` (String) AWS Account where this resource is managed
- `region` (String) Region where this resource is managed

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = awscc_emr_step.example
  id = "id"
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
$ terraform import awscc_emr_step.example "id"
```
