
---
page_title: "awscc_fsx_data_repository_association Resource - terraform-provider-awscc"
subcategory: ""
description: |-
  Resource Type definition for AWS::FSx::DataRepositoryAssociation
---

# awscc_fsx_data_repository_association (Resource)

Resource Type definition for AWS::FSx::DataRepositoryAssociation

## Example Usage

### FSx Lustre Data Repository Association with S3

Creates a data repository association between an FSx for Lustre file system and an S3 bucket, enabling automatic import and export of data with bi-directional synchronization for new, changed, and deleted files.

~> This example is generated by LLM using Amazon Bedrock and validated using terraform validate, apply and destroy. While we strive for accuracy and quality, please note that the information provided may not be entirely error-free or up-to-date. We recommend independently verifying the content.

```terraform
data "aws_region" "current" {}

data "aws_caller_identity" "current" {}

# VPC for FSx deployment
resource "awscc_ec2_vpc" "example" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}

resource "awscc_ec2_subnet" "example" {
  vpc_id            = awscc_ec2_vpc.example.id
  cidr_block        = "10.0.1.0/24"
  availability_zone = "${data.aws_region.current.name}a"

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}

resource "awscc_ec2_security_group" "example" {
  group_description = "FSx security group"
  vpc_id            = awscc_ec2_vpc.example.id

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}

# S3 bucket for FSx data repository
resource "awscc_s3_bucket" "fsx_data" {
  bucket_name = "fsx-dra-example-${data.aws_caller_identity.current.account_id}-${data.aws_region.current.name}"

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}

# FSx Lustre File System
resource "aws_fsx_lustre_file_system" "example" {
  storage_capacity            = 1200
  subnet_ids                  = [awscc_ec2_subnet.example.id]
  security_group_ids          = [awscc_ec2_security_group.example.id]
  deployment_type             = "PERSISTENT_1"
  per_unit_storage_throughput = 50

  tags = {
    "Modified By" = "AWSCC"
  }
}

# FSx Data Repository Association
resource "awscc_fsx_data_repository_association" "example" {
  file_system_id       = aws_fsx_lustre_file_system.example.id
  file_system_path     = "/data"
  data_repository_path = "s3://${awscc_s3_bucket.fsx_data.bucket_name}/data"

  batch_import_meta_data_on_create = true
  imported_file_chunk_size         = 1024

  s3 = {
    auto_import_policy = {
      events = ["NEW", "CHANGED", "DELETED"]
    }
    auto_export_policy = {
      events = ["NEW", "CHANGED", "DELETED"]
    }
  }

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `data_repository_path` (String) The path to the Amazon S3 data repository that will be linked to the file system. The path can be an S3 bucket or prefix in the format s3://myBucket/myPrefix/ . This path specifies where in the S3 data repository files will be imported from or exported to.
- `file_system_id` (String) The globally unique ID of the file system, assigned by Amazon FSx.
- `file_system_path` (String) This path specifies where in your file system files will be exported from or imported to. This file system directory can be linked to only one Amazon S3 bucket, and no other S3 bucket can be linked to the directory.

### Optional

- `batch_import_meta_data_on_create` (Boolean) A boolean flag indicating whether an import data repository task to import metadata should run after the data repository association is created. The task runs if this flag is set to true.
- `imported_file_chunk_size` (Number) For files imported from a data repository, this value determines the stripe count and maximum amount of data per file (in MiB) stored on a single physical disk. The maximum number of disks that a single file can be striped across is limited by the total number of disks that make up the file system.
- `s3` (Attributes) The configuration for an Amazon S3 data repository linked to an Amazon FSx Lustre file system with a data repository association. The configuration defines which file events (new, changed, or deleted files or directories) are automatically imported from the linked data repository to the file system or automatically exported from the file system to the data repository. (see [below for nested schema](#nestedatt--s3))
- `tags` (Attributes List) A list of Tag values, with a maximum of 50 elements. (see [below for nested schema](#nestedatt--tags))

### Read-Only

- `association_id` (String) The system-generated, unique ID of the data repository association.
- `id` (String) Uniquely identifies the resource.
- `resource_arn` (String) The Amazon Resource Name (ARN) for a given resource. ARNs uniquely identify Amazon Web Services resources. We require an ARN when you need to specify a resource unambiguously across all of Amazon Web Services. For more information, see Amazon Resource Names (ARNs) in the Amazon Web Services General Reference.

<a id="nestedatt--s3"></a>
### Nested Schema for `s3`

Optional:

- `auto_export_policy` (Attributes) Specifies the type of updated objects (new, changed, deleted) that will be automatically exported from your file system to the linked S3 bucket. (see [below for nested schema](#nestedatt--s3--auto_export_policy))
- `auto_import_policy` (Attributes) Specifies the type of updated objects (new, changed, deleted) that will be automatically imported from the linked S3 bucket to your file system. (see [below for nested schema](#nestedatt--s3--auto_import_policy))

<a id="nestedatt--s3--auto_export_policy"></a>
### Nested Schema for `s3.auto_export_policy`

Optional:

- `events` (Set of String)


<a id="nestedatt--s3--auto_import_policy"></a>
### Nested Schema for `s3.auto_import_policy`

Optional:

- `events` (Set of String)



<a id="nestedatt--tags"></a>
### Nested Schema for `tags`

Optional:

- `key` (String) The key name of the tag. You can specify a value that is 1 to 128 Unicode characters in length and cannot be prefixed with aws:. You can use any of the following characters: the set of Unicode letters, digits, whitespace, _, ., /, =, +, and -.
- `value` (String) The value for the tag. You can specify a value that is 0 to 256 Unicode characters in length and cannot be prefixed with aws:. You can use any of the following characters: the set of Unicode letters, digits, whitespace, _, ., /, =, +, and -.

## Import

Import is supported using the following syntax:

In Terraform v1.12.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `identity` attribute, for example:

```terraform
import {
  to = awscc_fsx_data_repository_association.example
  identity = {
    association_id = "association_id"
  }
}
```

<!-- schema generated by tfplugindocs -->
### Identity Schema

#### Required

- `association_id` (String) The system-generated, unique ID of the data repository association

#### Optional

- `account_id` (String) AWS Account where this resource is managed
- `region` (String) Region where this resource is managed

In Terraform v1.5.0 and later, the [`import` block](https://developer.hashicorp.com/terraform/language/import) can be used with the `id` attribute, for example:

```terraform
import {
  to = awscc_fsx_data_repository_association.example
  id = "association_id"
}
```

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
$ terraform import awscc_fsx_data_repository_association.example "association_id"
```
