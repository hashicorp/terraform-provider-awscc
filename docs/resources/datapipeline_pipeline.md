
---
page_title: "awscc_datapipeline_pipeline Resource - terraform-provider-awscc"
subcategory: ""
description: |-
  An example resource schema demonstrating some basic constructs and validation rules.
---

# awscc_datapipeline_pipeline (Resource)

An example resource schema demonstrating some basic constructs and validation rules.

## Example Usage

### AWS Data Pipeline with EC2 Shell Command

Creates an AWS Data Pipeline that executes a shell command on an EC2 instance, complete with necessary IAM roles, S3 logging bucket, and pipeline configuration parameters.

~> This example is generated by LLM using Amazon Bedrock and validated using terraform validate, apply and destroy. While we strive for accuracy and quality, please note that the information provided may not be entirely error-free or up-to-date. We recommend independently verifying the content.

```terraform
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}

resource "awscc_s3_bucket" "pipeline_logs" {
  bucket_name = "datapipeline-logs-${data.aws_caller_identity.current.account_id}-${data.aws_region.current.name}"

  public_access_block_configuration = {
    block_public_acls       = true
    block_public_policy     = true
    ignore_public_acls      = true
    restrict_public_buckets = true
  }

  tags = [
    {
      key   = "Modified By"
      value = "AWSCC"
    }
  ]
}

# Pipeline execution role
data "aws_iam_policy_document" "assume_role" {
  statement {
    effect = "Allow"
    principals {
      type = "Service"
      identifiers = [
        "datapipeline.amazonaws.com",
        "elasticmapreduce.amazonaws.com"
      ]
    }
    actions = ["sts:AssumeRole"]
  }
}

# Pipeline role policy
data "aws_iam_policy_document" "pipeline_role_policy" {
  statement {
    effect = "Allow"
    actions = [
      "s3:*",
      "ec2:*",
      "iam:PassRole",
      "iam:ListRolePolicies",
      "iam:GetRolePolicy",
      "iam:ListInstanceProfiles",
      "rds:Describe*",
      "redshift:DescribeClusters",
      "redshift:DescribeClusterSecurityGroups",
      "sns:*",
      "sqs:*",
      "cloudwatch:*",
      "elasticmapreduce:*"
    ]
    resources = ["*"]
  }
}

# EC2 instance role assume policy
data "aws_iam_policy_document" "ec2_assume_role" {
  statement {
    effect = "Allow"
    principals {
      type        = "Service"
      identifiers = ["ec2.amazonaws.com"]
    }
    actions = ["sts:AssumeRole"]
  }
}

# EC2 instance role policy
data "aws_iam_policy_document" "ec2_role_policy" {
  statement {
    effect = "Allow"
    actions = [
      "s3:*",
      "cloudwatch:*",
      "datapipeline:*",
      "dynamodb:*",
      "ec2:Describe*",
      "elasticmapreduce:AddJobFlowSteps",
      "elasticmapreduce:Describe*",
      "elasticmapreduce:ListInstance*",
      "rds:Describe*",
      "redshift:DescribeClusters",
      "redshift:DescribeClusterSecurityGroups",
      "sdb:*",
      "sns:*",
      "sqs:*"
    ]
    resources = ["*"]
  }
}

resource "awscc_iam_role" "pipeline_role" {
  role_name                   = "DataPipelineExampleRole"
  assume_role_policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.assume_role.json))
  policies = [
    {
      policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.pipeline_role_policy.json))
      policy_name     = "DataPipelineExamplePolicy"
    }
  ]
}

resource "awscc_iam_role" "ec2_role" {
  role_name                   = "DataPipelineEC2ExampleRole"
  assume_role_policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.ec2_assume_role.json))
  policies = [
    {
      policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.ec2_role_policy.json))
      policy_name     = "DataPipelineEC2ExamplePolicy"
    }
  ]
}

# Create the instance profile - name must match the role name
resource "awscc_iam_instance_profile" "ec2_profile" {
  instance_profile_name = awscc_iam_role.ec2_role.role_name
  roles                 = [awscc_iam_role.ec2_role.role_name]
}

resource "awscc_datapipeline_pipeline" "example" {
  name        = "example-pipeline"
  description = "Example Data Pipeline using AWSCC provider"

  parameter_objects = [
    {
      id = "myShellCmd"
      attributes = [
        {
          key          = "type"
          string_value = "String"
        },
        {
          key          = "description"
          string_value = "Shell command to run"
        }
      ]
    }
  ]

  parameter_values = [
    {
      id           = "myShellCmd"
      string_value = "echo 'Hello from Data Pipeline'"
    }
  ]

  pipeline_objects = [
    {
      id   = "Default"
      name = "Default"
      fields = [
        {
          key          = "type"
          string_value = "Default"
        },
        {
          key          = "scheduleType"
          string_value = "ondemand"
        },
        {
          key          = "role"
          string_value = awscc_iam_role.pipeline_role.role_name
        },
        {
          key          = "pipelineLogUri"
          string_value = "s3://${awscc_s3_bucket.pipeline_logs.bucket_name}/logs/"
        }
      ]
    },
    {
      id   = "MyEC2Resource"
      name = "MyEC2Resource"
      fields = [
        {
          key          = "type"
          string_value = "Ec2Resource"
        },
        {
          key          = "terminateAfter"
          string_value = "1 Hour"
        },
        {
          key          = "role"
          string_value = awscc_iam_role.pipeline_role.role_name
        },
        {
          key          = "resourceRole"
          string_value = awscc_iam_role.ec2_role.role_name
        }
      ]
    },
    {
      id   = "ShellCommandActivity"
      name = "ShellCommandActivity"
      fields = [
        {
          key          = "type"
          string_value = "ShellCommandActivity"
        },
        {
          key       = "runsOn"
          ref_value = "MyEC2Resource"
        },
        {
          key          = "command"
          string_value = "#{myShellCmd}"
        }
      ]
    }
  ]

  pipeline_tags = [
    {
      key   = "Environment"
      value = "Development"
    },
    {
      key   = "Modified By"
      value = "AWSCC"
    }
  ]

  # Set to true to activate the pipeline immediately
  activate = false

  depends_on = [
    awscc_s3_bucket.pipeline_logs,
    awscc_iam_instance_profile.ec2_profile
  ]
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `name` (String) The name of the pipeline.

### Optional

- `activate` (Boolean) Indicates whether to validate and start the pipeline or stop an active pipeline. By default, the value is set to true.
- `description` (String) A description of the pipeline.
- `parameter_objects` (Attributes List) The parameter objects used with the pipeline. (see [below for nested schema](#nestedatt--parameter_objects))
- `parameter_values` (Attributes List) The parameter values used with the pipeline. (see [below for nested schema](#nestedatt--parameter_values))
- `pipeline_objects` (Attributes List) The objects that define the pipeline. These objects overwrite the existing pipeline definition. Not all objects, fields, and values can be updated. For information about restrictions, see Editing Your Pipeline in the AWS Data Pipeline Developer Guide. (see [below for nested schema](#nestedatt--pipeline_objects))
- `pipeline_tags` (Attributes List) A list of arbitrary tags (key-value pairs) to associate with the pipeline, which you can use to control permissions. For more information, see Controlling Access to Pipelines and Resources in the AWS Data Pipeline Developer Guide. (see [below for nested schema](#nestedatt--pipeline_tags))

### Read-Only

- `id` (String) Uniquely identifies the resource.
- `pipeline_id` (String)

<a id="nestedatt--parameter_objects"></a>
### Nested Schema for `parameter_objects`

Optional:

- `attributes` (Attributes List) The attributes of the parameter object. (see [below for nested schema](#nestedatt--parameter_objects--attributes))
- `id` (String) The ID of the parameter object.

<a id="nestedatt--parameter_objects--attributes"></a>
### Nested Schema for `parameter_objects.attributes`

Optional:

- `key` (String) The field identifier.
- `string_value` (String) The field value, expressed as a String.



<a id="nestedatt--parameter_values"></a>
### Nested Schema for `parameter_values`

Optional:

- `id` (String) The ID of the parameter value.
- `string_value` (String) The field value, expressed as a String.


<a id="nestedatt--pipeline_objects"></a>
### Nested Schema for `pipeline_objects`

Optional:

- `fields` (Attributes List) Key-value pairs that define the properties of the object. (see [below for nested schema](#nestedatt--pipeline_objects--fields))
- `id` (String) The ID of the object.
- `name` (String) The name of the object.

<a id="nestedatt--pipeline_objects--fields"></a>
### Nested Schema for `pipeline_objects.fields`

Optional:

- `key` (String) Specifies the name of a field for a particular object. To view valid values for a particular field, see Pipeline Object Reference in the AWS Data Pipeline Developer Guide.
- `ref_value` (String) A field value that you specify as an identifier of another object in the same pipeline definition.
- `string_value` (String) A field value that you specify as a string. To view valid values for a particular field, see Pipeline Object Reference in the AWS Data Pipeline Developer Guide.



<a id="nestedatt--pipeline_tags"></a>
### Nested Schema for `pipeline_tags`

Optional:

- `key` (String) The key name of a tag.
- `value` (String) The value to associate with the key name.

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
$ terraform import awscc_datapipeline_pipeline.example "pipeline_id"
```
