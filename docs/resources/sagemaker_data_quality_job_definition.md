
---
page_title: "awscc_sagemaker_data_quality_job_definition Resource - terraform-provider-awscc"
subcategory: ""
description: |-
  Resource Type definition for AWS::SageMaker::DataQualityJobDefinition
---

# awscc_sagemaker_data_quality_job_definition (Resource)

Resource Type definition for AWS::SageMaker::DataQualityJobDefinition

## Example Usage

### Configure SageMaker Data Quality Monitoring Job

Creates a SageMaker Data Quality Job Definition with monitoring configuration for batch transform input, including necessary IAM roles and S3 bucket for storing monitoring data and results.

~> This example is generated by LLM using Amazon Bedrock and validated using terraform validate, apply and destroy. While we strive for accuracy and quality, please note that the information provided may not be entirely error-free or up-to-date. We recommend independently verifying the content.

```terraform
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}

# Create S3 bucket for outputs
resource "awscc_s3_bucket" "monitoring" {
  bucket_name = "sagemaker-dataquality-${data.aws_caller_identity.current.account_id}-${data.aws_region.current.name}"
  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}

# IAM role for SageMaker
data "aws_iam_policy_document" "assume_role" {
  statement {
    actions = ["sts:AssumeRole"]
    principals {
      type        = "Service"
      identifiers = ["sagemaker.amazonaws.com"]
    }
  }
}

data "aws_iam_policy_document" "sagemaker_policy" {
  statement {
    effect = "Allow"
    actions = [
      "s3:GetObject",
      "s3:PutObject",
      "s3:ListBucket"
    ]
    resources = [
      "arn:aws:s3:::${awscc_s3_bucket.monitoring.bucket_name}",
      "arn:aws:s3:::${awscc_s3_bucket.monitoring.bucket_name}/*"
    ]
  }
  statement {
    effect = "Allow"
    actions = [
      "logs:CreateLogGroup",
      "logs:CreateLogStream",
      "logs:PutLogEvents"
    ]
    resources = ["arn:aws:logs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:log-group:/aws/sagemaker/*"]
  }
}

resource "awscc_iam_role" "sagemaker_role" {
  assume_role_policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.assume_role.json))
  description                 = "IAM role for SageMaker Data Quality Job Definition"
  policies = [{
    policy_document = jsonencode(jsondecode(data.aws_iam_policy_document.sagemaker_policy.json))
    policy_name     = "SageMakerDataQualityPolicy"
  }]
  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}

# Data Quality Job Definition
resource "awscc_sagemaker_data_quality_job_definition" "example" {
  job_definition_name = "example-data-quality-job"

  data_quality_app_specification = {
    image_uri = "174368400705.dkr.ecr.${data.aws_region.current.name}.amazonaws.com/sagemaker-model-monitor-analyzer:latest"
  }

  data_quality_job_input = {
    batch_transform_input = {
      data_captured_destination_s3_uri = "s3://${awscc_s3_bucket.monitoring.bucket_name}/input"
      local_path                       = "/opt/ml/processing/input"
      s3_data_distribution_type        = "FullyReplicated"
      s3_input_mode                    = "File"
      dataset_format = {
        csv = {
          header = true
        }
      }
    }
  }

  data_quality_job_output_config = {
    monitoring_outputs = [{
      s3_output = {
        local_path     = "/opt/ml/processing/output"
        s3_uri         = "s3://${awscc_s3_bucket.monitoring.bucket_name}/output"
        s3_upload_mode = "EndOfJob"
      }
    }]
  }

  job_resources = {
    cluster_config = {
      instance_count    = 1
      instance_type     = "ml.m5.large"
      volume_size_in_gb = 20
    }
  }

  role_arn = awscc_iam_role.sagemaker_role.arn

  stopping_condition = {
    max_runtime_in_seconds = 3600
  }

  tags = [{
    key   = "Modified By"
    value = "AWSCC"
  }]
}
```

<!-- schema generated by tfplugindocs -->
## Schema

### Required

- `data_quality_app_specification` (Attributes) Container image configuration object for the monitoring job. (see [below for nested schema](#nestedatt--data_quality_app_specification))
- `data_quality_job_input` (Attributes) The inputs for a monitoring job. (see [below for nested schema](#nestedatt--data_quality_job_input))
- `data_quality_job_output_config` (Attributes) The output configuration for monitoring jobs. (see [below for nested schema](#nestedatt--data_quality_job_output_config))
- `job_resources` (Attributes) Identifies the resources to deploy for a monitoring job. (see [below for nested schema](#nestedatt--job_resources))
- `role_arn` (String) The Amazon Resource Name (ARN) of an IAM role that Amazon SageMaker can assume to perform tasks on your behalf.

### Optional

- `data_quality_baseline_config` (Attributes) Baseline configuration used to validate that the data conforms to the specified constraints and statistics. (see [below for nested schema](#nestedatt--data_quality_baseline_config))
- `endpoint_name` (String) The name of the endpoint used to run the monitoring job.
- `job_definition_name` (String) The name of the job definition.
- `network_config` (Attributes) Networking options for a job, such as network traffic encryption between containers, whether to allow inbound and outbound network calls to and from containers, and the VPC subnets and security groups to use for VPC-enabled jobs. (see [below for nested schema](#nestedatt--network_config))
- `stopping_condition` (Attributes) Specifies a time limit for how long the monitoring job is allowed to run. (see [below for nested schema](#nestedatt--stopping_condition))
- `tags` (Attributes List) An array of key-value pairs to apply to this resource. (see [below for nested schema](#nestedatt--tags))

### Read-Only

- `creation_time` (String) The time at which the job definition was created.
- `id` (String) Uniquely identifies the resource.
- `job_definition_arn` (String) The Amazon Resource Name (ARN) of job definition.

<a id="nestedatt--data_quality_app_specification"></a>
### Nested Schema for `data_quality_app_specification`

Required:

- `image_uri` (String) The container image to be run by the monitoring job.

Optional:

- `container_arguments` (List of String) An array of arguments for the container used to run the monitoring job.
- `container_entrypoint` (List of String) Specifies the entrypoint for a container used to run the monitoring job.
- `environment` (Map of String) Sets the environment variables in the Docker container
- `post_analytics_processor_source_uri` (String) An Amazon S3 URI to a script that is called after analysis has been performed. Applicable only for the built-in (first party) containers.
- `record_preprocessor_source_uri` (String) An Amazon S3 URI to a script that is called per row prior to running analysis. It can base64 decode the payload and convert it into a flatted json so that the built-in container can use the converted data. Applicable only for the built-in (first party) containers


<a id="nestedatt--data_quality_job_input"></a>
### Nested Schema for `data_quality_job_input`

Optional:

- `batch_transform_input` (Attributes) The batch transform input for a monitoring job. (see [below for nested schema](#nestedatt--data_quality_job_input--batch_transform_input))
- `endpoint_input` (Attributes) The endpoint for a monitoring job. (see [below for nested schema](#nestedatt--data_quality_job_input--endpoint_input))

<a id="nestedatt--data_quality_job_input--batch_transform_input"></a>
### Nested Schema for `data_quality_job_input.batch_transform_input`

Optional:

- `data_captured_destination_s3_uri` (String) A URI that identifies the Amazon S3 storage location where Batch Transform Job captures data.
- `dataset_format` (Attributes) The dataset format of the data to monitor (see [below for nested schema](#nestedatt--data_quality_job_input--batch_transform_input--dataset_format))
- `exclude_features_attribute` (String) Indexes or names of the features to be excluded from analysis
- `local_path` (String) Path to the filesystem where the endpoint data is available to the container.
- `s3_data_distribution_type` (String) Whether input data distributed in Amazon S3 is fully replicated or sharded by an S3 key. Defauts to FullyReplicated
- `s3_input_mode` (String) Whether the Pipe or File is used as the input mode for transfering data for the monitoring job. Pipe mode is recommended for large datasets. File mode is useful for small files that fit in memory. Defaults to File.

<a id="nestedatt--data_quality_job_input--batch_transform_input--dataset_format"></a>
### Nested Schema for `data_quality_job_input.batch_transform_input.dataset_format`

Optional:

- `csv` (Attributes) The CSV format (see [below for nested schema](#nestedatt--data_quality_job_input--batch_transform_input--dataset_format--csv))
- `json` (Attributes) The Json format (see [below for nested schema](#nestedatt--data_quality_job_input--batch_transform_input--dataset_format--json))
- `parquet` (Boolean) A flag indicate if the dataset format is Parquet

<a id="nestedatt--data_quality_job_input--batch_transform_input--dataset_format--csv"></a>
### Nested Schema for `data_quality_job_input.batch_transform_input.dataset_format.csv`

Optional:

- `header` (Boolean) A boolean flag indicating if given CSV has header


<a id="nestedatt--data_quality_job_input--batch_transform_input--dataset_format--json"></a>
### Nested Schema for `data_quality_job_input.batch_transform_input.dataset_format.json`

Optional:

- `line` (Boolean) A boolean flag indicating if it is JSON line format




<a id="nestedatt--data_quality_job_input--endpoint_input"></a>
### Nested Schema for `data_quality_job_input.endpoint_input`

Optional:

- `endpoint_name` (String) The name of the endpoint used to run the monitoring job.
- `exclude_features_attribute` (String) Indexes or names of the features to be excluded from analysis
- `local_path` (String) Path to the filesystem where the endpoint data is available to the container.
- `s3_data_distribution_type` (String) Whether input data distributed in Amazon S3 is fully replicated or sharded by an S3 key. Defauts to FullyReplicated
- `s3_input_mode` (String) Whether the Pipe or File is used as the input mode for transfering data for the monitoring job. Pipe mode is recommended for large datasets. File mode is useful for small files that fit in memory. Defaults to File.



<a id="nestedatt--data_quality_job_output_config"></a>
### Nested Schema for `data_quality_job_output_config`

Required:

- `monitoring_outputs` (Attributes List) Monitoring outputs for monitoring jobs. This is where the output of the periodic monitoring jobs is uploaded. (see [below for nested schema](#nestedatt--data_quality_job_output_config--monitoring_outputs))

Optional:

- `kms_key_id` (String) The AWS Key Management Service (AWS KMS) key that Amazon SageMaker uses to encrypt the model artifacts at rest using Amazon S3 server-side encryption.

<a id="nestedatt--data_quality_job_output_config--monitoring_outputs"></a>
### Nested Schema for `data_quality_job_output_config.monitoring_outputs`

Required:

- `s3_output` (Attributes) Information about where and how to store the results of a monitoring job. (see [below for nested schema](#nestedatt--data_quality_job_output_config--monitoring_outputs--s3_output))

<a id="nestedatt--data_quality_job_output_config--monitoring_outputs--s3_output"></a>
### Nested Schema for `data_quality_job_output_config.monitoring_outputs.s3_output`

Required:

- `local_path` (String) The local path to the Amazon S3 storage location where Amazon SageMaker saves the results of a monitoring job. LocalPath is an absolute path for the output data.
- `s3_uri` (String) A URI that identifies the Amazon S3 storage location where Amazon SageMaker saves the results of a monitoring job.

Optional:

- `s3_upload_mode` (String) Whether to upload the results of the monitoring job continuously or after the job completes.




<a id="nestedatt--job_resources"></a>
### Nested Schema for `job_resources`

Required:

- `cluster_config` (Attributes) Configuration for the cluster used to run model monitoring jobs. (see [below for nested schema](#nestedatt--job_resources--cluster_config))

<a id="nestedatt--job_resources--cluster_config"></a>
### Nested Schema for `job_resources.cluster_config`

Required:

- `instance_count` (Number) The number of ML compute instances to use in the model monitoring job. For distributed processing jobs, specify a value greater than 1. The default value is 1.
- `instance_type` (String) The ML compute instance type for the processing job.
- `volume_size_in_gb` (Number) The size of the ML storage volume, in gigabytes, that you want to provision. You must specify sufficient ML storage for your scenario.

Optional:

- `volume_kms_key_id` (String) The AWS Key Management Service (AWS KMS) key that Amazon SageMaker uses to encrypt data on the storage volume attached to the ML compute instance(s) that run the model monitoring job.



<a id="nestedatt--data_quality_baseline_config"></a>
### Nested Schema for `data_quality_baseline_config`

Optional:

- `baselining_job_name` (String) The name of a processing job
- `constraints_resource` (Attributes) The baseline constraints resource for a monitoring job. (see [below for nested schema](#nestedatt--data_quality_baseline_config--constraints_resource))
- `statistics_resource` (Attributes) The baseline statistics resource for a monitoring job. (see [below for nested schema](#nestedatt--data_quality_baseline_config--statistics_resource))

<a id="nestedatt--data_quality_baseline_config--constraints_resource"></a>
### Nested Schema for `data_quality_baseline_config.constraints_resource`

Optional:

- `s3_uri` (String) The Amazon S3 URI for baseline constraint file in Amazon S3 that the current monitoring job should validated against.


<a id="nestedatt--data_quality_baseline_config--statistics_resource"></a>
### Nested Schema for `data_quality_baseline_config.statistics_resource`

Optional:

- `s3_uri` (String) The Amazon S3 URI for the baseline statistics file in Amazon S3 that the current monitoring job should be validated against.



<a id="nestedatt--network_config"></a>
### Nested Schema for `network_config`

Optional:

- `enable_inter_container_traffic_encryption` (Boolean) Whether to encrypt all communications between distributed processing jobs. Choose True to encrypt communications. Encryption provides greater security for distributed processing jobs, but the processing might take longer.
- `enable_network_isolation` (Boolean) Whether to allow inbound and outbound network calls to and from the containers used for the processing job.
- `vpc_config` (Attributes) Specifies a VPC that your training jobs and hosted models have access to. Control access to and from your training and model containers by configuring the VPC. (see [below for nested schema](#nestedatt--network_config--vpc_config))

<a id="nestedatt--network_config--vpc_config"></a>
### Nested Schema for `network_config.vpc_config`

Optional:

- `security_group_ids` (List of String) The VPC security group IDs, in the form sg-xxxxxxxx. Specify the security groups for the VPC that is specified in the Subnets field.
- `subnets` (List of String) The ID of the subnets in the VPC to which you want to connect to your monitoring jobs.



<a id="nestedatt--stopping_condition"></a>
### Nested Schema for `stopping_condition`

Optional:

- `max_runtime_in_seconds` (Number) The maximum runtime allowed in seconds.


<a id="nestedatt--tags"></a>
### Nested Schema for `tags`

Optional:

- `key` (String) The key name of the tag. You can specify a value that is 1 to 127 Unicode characters in length and cannot be prefixed with aws:. You can use any of the following characters: the set of Unicode letters, digits, whitespace, _, ., /, =, +, and -.
- `value` (String) The value for the tag. You can specify a value that is 1 to 255 Unicode characters in length and cannot be prefixed with aws:. You can use any of the following characters: the set of Unicode letters, digits, whitespace, _, ., /, =, +, and -.

## Import

Import is supported using the following syntax:

The [`terraform import` command](https://developer.hashicorp.com/terraform/cli/commands/import) can be used, for example:

```shell
$ terraform import awscc_sagemaker_data_quality_job_definition.example "job_definition_arn"
```
